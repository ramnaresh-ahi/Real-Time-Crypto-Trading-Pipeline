# Real-Time Crypto Trading Pipeline

> **âš ï¸ Project Status: Incomplete - On Hold**
> 
> This project is currently incomplete and has been paused due to time constraints and complexity. I plan to resume and complete it in the future when time permits. The current implementation demonstrates a working foundation for real-time data streaming and processing.

## ğŸ“‹ Project Overview

A comprehensive real-time data engineering pipeline designed to ingest, process, and analyze cryptocurrency trading data using modern streaming technologies. The project demonstrates end-to-end data engineering principles including real-time streaming, data quality monitoring, and workflow orchestration.

## ğŸ¯ Project Goal

Build a production-grade, scalable real-time data pipeline that:
- Ingests live cryptocurrency trade data via streaming APIs
- Processes and transforms data in real-time using Apache Kafka
- Stores raw trades in MongoDB for flexibility
- Aggregates and stores analytical data in PostgreSQL
- Orchestrates workflows and data quality checks using Apache Airflow
- Provides monitoring and visualization capabilities

## ğŸ—ï¸ Architecture

```
[Binance WebSocket API] 
        â†“
    [Producer]
        â†“
   [Apache Kafka]
        â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
    â†“               â†“
[MongoDB Consumer] [Postgres Consumer]
    â†“               â†“
[MongoDB]       [PostgreSQL]
    â†“               â†“
[Apache Airflow DAGs - Data Quality & Orchestration]
```

## ğŸ› ï¸ Technology Stack

### Core Technologies
- **Apache Kafka** (v7.4.0) - Message broker for real-time streaming
- **Apache Zookeeper** (v7.4.0) - Kafka coordination service
- **MongoDB** (v6.0) - NoSQL database for raw trade storage
- **PostgreSQL** (v13) - Relational database for aggregated analytics
- **Apache Airflow** (v2.7.0) - Workflow orchestration and scheduling
- **Docker & Docker Compose** - Containerization and service orchestration

### Python Libraries
- `kafka-python` - Kafka client
- `pymongo` - MongoDB driver
- `psycopg2-binary` - PostgreSQL adapter
- `websockets` - WebSocket client for real-time data
- `python-dotenv` - Environment configuration
- `streamlit` & `plotly` - Dashboard and visualization (planned)

## ğŸ“ Project Structure

```
Real-Time-Crypto-Trading-Pipeline/
â”œâ”€â”€ airflow/                    # Airflow configuration and metadata
â”‚   â”œâ”€â”€ postgres_data/         # Airflow metadata database
â”‚   â”œâ”€â”€ airflow.cfg            # Airflow configuration
â”‚   â””â”€â”€ webserver_config.py    # Webserver settings
â”œâ”€â”€ config/                    # Configuration files
â”‚   â”œâ”€â”€ .env                   # Environment variables
â”‚   â””â”€â”€ json_schemas/          # Data validation schemas
â”œâ”€â”€ dags/                      # Airflow DAG definitions
â”‚   â””â”€â”€ data_quality_dag.py    # Data quality monitoring DAG
â”œâ”€â”€ dashboard/                 # Monitoring dashboard (planned)
â”‚   â””â”€â”€ dashboard.py
â”œâ”€â”€ mongo_consumer/            # MongoDB Kafka consumer
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ mongo_consumer.py
â”œâ”€â”€ pg_consumer/               # PostgreSQL Kafka consumer
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ pg_consumer.py
â”œâ”€â”€ producer/                  # Kafka message producer
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ producer.py
â”œâ”€â”€ scripts/                   # Utility scripts
â”‚   â””â”€â”€ init_mongo.sh          # MongoDB initialization
â”œâ”€â”€ tests/                     # Test suites
â”‚   â”œâ”€â”€ test_mongo_loader.py
â”‚   â”œâ”€â”€ test_producer.py
â”‚   â””â”€â”€ test_transformer.py
â”œâ”€â”€ transformer/               # Data transformation logic
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ transformer.py
â”œâ”€â”€ validation/                # Schema validation
â”‚   â””â”€â”€ trade_event.json       # Trade event JSON schema
â”œâ”€â”€ docker-compose.yml         # Docker services configuration
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                  # Project documentation
```

## âœ… What Has Been Completed

### 1. Infrastructure Setup
- [âœ”] Docker Compose configuration for all services
- [âœ”] Custom Docker network (`rt-crypto-net`) for inter-container communication
- [âœ”] Service dependencies and health checks configured

### 2. Data Ingestion Layer
- [âœ”] Kafka broker and Zookeeper setup
- [âœ”] Kafka UI for monitoring (accessible at `localhost:8081`)
- [âœ”] Producer implementation for streaming trade data
- [âœ”] WebSocket integration for real-time data ingestion

### 3. Data Processing Layer
- [âœ”] MongoDB consumer implementation
  - Successfully consuming messages from Kafka topics
  - Storing raw trades in `trades_raw` collection
  - Minimal consumer lag achieved
- [âœ”] PostgreSQL consumer implementation
  - Consuming and aggregating trade data
  - Storing aggregated metrics in `trades_agg` table
  - Working with minimal lag

### 4. Storage Layer
- [âœ”] MongoDB replica set configuration
- [âœ”] PostgreSQL database setup with Airflow integration
- [âœ”] Database connection strings and environment variables configured

### 5. Orchestration Layer (Partial)
- [âœ”] Airflow scheduler setup
- [âœ”] Basic data quality DAG created
- [âœ”] DAG file structure and imports configured
- [âœ”] Requirements.txt with all necessary dependencies
- [âœ”] Webserver fully operational
- [âœ”] Complete DAG implementation with real data quality checks
- [âœ”] Email notifications and alerting configured
- [âœ”] Scheduled batch processing jobs


### 6. Monitoring
- [âœ”] Kafka UI for consumer lag monitoring
- [âœ”] Docker container health monitoring
- [âœ”] Consumer group metrics tracking

## ğŸš§ Known Issues & Incomplete Work

### Issues Encountered

1. **Airflow Webserver Timeout**
   - Gunicorn master not responding within 120 seconds
   - Webserver container exits after timeout
   - Scheduler runs successfully but webserver fails to start completely

2. **Python Dependencies in Airflow**
   - `pymongo` and `psycopg2-binary` installation challenges in Airflow containers
   - Requirements.txt mounting and installation timing issues

3. **Resource Constraints**
   - Worker timeout errors in Airflow scheduler
   - Potential memory exhaustion (SIGKILL signals observed)

### Incomplete Components

1.. **Data Quality & Validation**
   - [ ] JSON schema validation in producer/consumers
   - [ ] Data quality checks integrated into DAGs
   - [ ] Monitoring for duplicates and schema drift
   - [ ] Automated data quality tests

2. **Monitoring & Visualization**
   - [ ] Dashboard implementation with Streamlit
   - [ ] Prometheus/Grafana integration for metrics
   - [ ] Custom alerting system
   - [ ] Performance monitoring dashboards

3. **Testing**
   - [ ] Comprehensive unit tests for all components
   - [ ] Integration tests for end-to-end flow
   - [ ] Contract tests for schema conformance
   - [ ] CI/CD pipeline setup

4. **Documentation**
   - [ ] Setup instructions and prerequisites
   - [ ] Troubleshooting guide
   - [ ] Architecture diagrams
   - [ ] API documentation

5. **Security**
   - [ ] Authentication on MongoDB and Kafka
   - [ ] Encryption for data in transit
   - [ ] Secrets management
   - [ ] Access control lists

## ğŸš€ Quick Start (Current State)

### Prerequisites
- Docker Desktop installed and running
- At least 8GB RAM available
- Ports available: 8080, 8081, 9092, 27017, 5432

### Running the Pipeline

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd Real-Time-Crypto-Trading-Pipeline
   ```

2. **Create virtual environment & activate it**
   ```bash
   python -m venv myvenv
   ```

   ```bash
   myvenv\Scripts\activate
   ```

3. **Install required dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Start services**
   ```bash
   docker-compose up -d
   ```

5. **Verify running services**
   ```bash
   docker ps
   ```

6. **Run kafka producer**
   ```bash
   python producer/producer.py
   ```

7. **Access Kafka UI & airflow**
   - Open browser: `http://localhost:8081`
   - Monitor consumer groups and message flow

   - Open browser: `http://localhost:8080`
   - Monitor crypto_data_quality_check dag

8. **Check consumer logs**
   ```bash
   docker logs mongo-consumer -f
   docker logs pg-consumer -f
   ```

### Stopping the Pipeline

```bash
docker-compose down
```

To remove volumes and clean up completely:
```bash
docker-compose down -v
```

## ğŸ“Š Current Data Flow

1. **Producer** â†’ Generates/streams trade messages to Kafka topic
2. **Kafka** â†’ Distributes messages to consumer groups
3. **MongoDB Consumer** â†’ Reads from Kafka, stores raw trades in MongoDB
4. **Postgres Consumer** â†’ Reads from Kafka, aggregates and stores in PostgreSQL
5. **Kafka UI** â†’ Monitors consumer lag and message throughput
6. **Airflow UI** â†’ Monitor crypto_data_quality_check dag

### Verified Working
- âœ… Producer successfully producing messages
- âœ… Consumers successfully consuming with minimal lag (typically <100 messages)
- âœ… Data flowing into both MongoDB and PostgreSQL
- âœ… Consumer groups stable and healthy
- âœ… Network connectivity between all containers

## ğŸ”§ Configuration

### Environment Variables

Key configurations in `config/.env`, `airflow/airflow.cfg` and `docker-compose.yml`:

```yaml
# Kafka
KAFKA_BOOTSTRAP_SERVERS=kafka:9092

# MongoDB
MONGO_URI=mongodb://mongodb:27017
MONGO_DB=crypto
MONGO_COLLECTION=trades_raw

# PostgreSQL
POSTGRES_CONN=host=postgres port=5432 dbname=airflow user=airflow password=airflow

# Airflow
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
```

## ğŸ“ Lessons Learned

1. **Docker Networking**: Custom bridge networks enable reliable DNS-based service discovery
2. **Consumer Lag Management**: Proper Kafka partitioning and consumer group configuration is critical
3. **Resource Management**: Airflow requires careful tuning of worker timeouts and memory limits
4. **Dependency Management**: Installing Python packages in Docker requires proper requirements file mounting and build steps
5. **Modular Architecture**: Separation of concerns across folders aids maintainability and debugging

## ğŸ¯ Future Roadmap (When Resuming)

### Phase 1: Data Transformation
- Handle missing values
- Data types
- Naming convention

### Phase 2: Data Quality
- Implement schema validation
- Add data quality tests in DAGs
- Set up monitoring and alerts

### Phase 3: Visualization
- Complete dashboard implementation
- Add real-time metrics visualization
- Integrate Grafana for monitoring

### Phase 4: Testing & CI/CD
- Write comprehensive test suites
- Set up GitHub Actions for CI/CD
- Add automated deployment

### Phase 5: Production Hardening
- Add security layers (auth, encryption)
- Implement backup and disaster recovery
- Performance optimization and scaling

## ğŸ¤ Contributing

This project is currently on hold and not accepting contributions. However, feel free to fork and experiment with the codebase for learning purposes.

## ğŸ“œ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ‘¤ Author

**Your Name**
- GitHub: [@ramnaresh-ahi](https://github.com/ramnaresh-ahi)
- LinkedIn: [Ramnaresh Ahirwar](https://www.linkedin.com/in/ramnaresh-ahirwar-77abc/)

## ğŸ™ Acknowledgments

- Binance API for providing real-time cryptocurrency data
- Apache Kafka, MongoDB, PostgreSQL, and Airflow communities
- Docker and containerization ecosystem

---

## ğŸ“Œ Why This Project is On Hold

This project has been temporarily paused due to:

1. **Time Constraints**: The project requires significant time investment for debugging, testing, and completing remaining features
2. **Complexity**: Real-time data engineering with multiple technologies requires careful orchestration and troubleshooting
3. **Resource Requirements**: Running all services simultaneously demands substantial local system resources
4. **Learning Curve**: Some technical challenges (especially Airflow configuration) require deeper investigation

**I will continue this project in the future** when I have more dedicated time to properly complete the implementation, testing, and documentation.

### Current Status Summary
- âœ… Core streaming pipeline working (Producer â†’ Kafka â†’ Consumers â†’ Databases)
- âœ… Airflow orchestration working (crypto_data_quality_check dag, email alert)
- âŒ Monitoring, testing, and security features incomplete

Despite being incomplete, this project demonstrates:
- Real-time data streaming architecture
- Microservices design with Docker
- Kafka consumer implementation
- Database integration (NoSQL and SQL)
- Modern data engineering practices

Thank you for your interest in this project! Stars â­ and feedback are appreciated.

---

*Last Updated: October 16, 2025*
